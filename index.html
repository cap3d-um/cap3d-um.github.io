
<!DOCTYPE html>
<html lang="en">
<head>
    <title>Cap3D</title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="shortcut icon" href="static/img/favicon.ico">

    <!--[if lt IE 9]>
      <script src="http://cdn.bootcss.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="http://cdn.bootcss.com/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <style type="text/css">
body{font-family:"Open Sans",Segoe,"Segoe UI","Lucida Sans Unicode","Lucida Grande","Avenir","Seravek","Ubuntu","DejaVu Sans","Trebuchet MS",Verdana,Arial,sans-serif}
#header{background:#00274C;opacity:0.95;margin:0 auto;padding:50px 0 0;text-align:center;cursor:default;text-align:center}
#header-container{margin:0 auto;padding: 0 2em;max-width:1000px}

#header-container .col-logo{text-align:left}
#header-container .logo{position:relative;z-index:100;height:50px;margin-top:20px;margin-right:10px}

#header-container .col-info{text-align:left;margin-bottom:20px}
#header-container #title{margin:.525em 0 1.525em;font-size:1.6em;font-weight:800;text-align:center}

.paper-info{color:#8a8989;display:inline-block;margin:0 auto;text-align:left}
.paper-info-margin{margin-bottom:20px}
.paper-info p, .paper-info h3{line-height:1.6em; margin-bottom: 0em}
.paper-info .title{font-size:16px;color:#FFCB05;font-weight:600}
.paper-info .authors, .paper-info .authors a{color:#989C97}
.paper-info .authors_bottom, .paper-info .authors_bottom a{color:#567EAE}
.paper-info .email{color:#666; font-size:14px}
.paper-info .tag{margin:auto auto;padding:0;list-style:none;text-align:left}
.paper-info .tag li{display:inline-block;margin:auto;padding:0 3px 0 0;line-height:10px;color:#567EAE} a{color:#567EAE}
.paper-info .conference, .paper-info .conference a{color:#FFCB05;font-weight:600}
.paper-info .top a{color:#989C97}

#wave-canvas{color:#8a8989;display:block;margin:-80px 0 0;width:100%;height:150px}

#content{padding-top:0px;text-align:left;}
#content-container{margin:0 auto;padding: 0 2em;max-width:1000px;}

#content-container .header{color:#00274C;background:#FFCB05;padding:15px 30px 5px;border-bottom:3px solid #dddddd}
#content-container .header .indicator{color:#00274C;margin-right:12px}

#content-container .content{background:#ffffff;padding:15px 5px 15px 30px}
#content-container .content .caption{color:#989C97}
#content-container .content .bib{color:#989C97;font-size:14px;}
#content-container .content .bib pre{margin:0;padding:0;}
#content-container .content .href{style="color:#567EAE"}



#footer{padding:2em 0 0.5em;margin:30px 0 0;background:#ffffff;opacity:0.95;font-size:14px;line-height:12px;text-align:center;color:#989C97}
.highlight, .highlight a{color:#BB2222;font-weight:600}
    </style>
</head>
<body>
<div id="main">
    <div id="header">
        <div id="header-container" class="container">
          <div class="row">
            <div class="col-md-12 col-xl-12 col-info">
                <h1 id="title"><p style="color:#FFCB05"> Cap3D </p></h1>
                <div class="paper-info", style="margin-bottom: 15px;">
                    <h3 class="title">View Selection for 3D Captioning via Diffusion Ranking
</h3>
                    <p class="authors">
                    <a href="https://tiangeluo.github.io/">Tiange Luo</a>,   <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson<sup>&#8224;</sup></a>, <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee<sup>&#8224;</sup></a> (&#8224;: equal advising)
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="http://arxiv.org/abs/2404.07984">ECCV 2024</a> </li>
                        <li class="top"><a href="http://arxiv.org/abs/2404.07984">Paper</a> |</li>
                        <li class="top"><a href="https://github.com/tiangeluo/DiffuRank">Code</a> |</li>
                        <li class="top"><a href="https://huggingface.co/datasets/tiange/Cap3D">Dataset</a> |</li>
                        <li class="top"><a href="https://tiangeluo.github.io/projectpages/bibs/cap3d.txt">BibTeX</a> </li>
                    </ul>
                </div>
                <!-- <div class="spacer" style="height: 20px;"></div>  This div acts as a spacer -->
                <div class="paper-info">
                    <h3 class="title">Scalable 3D Captioning with Pretrained Models
</h3>
                    <p class="authors">
                    <a href="https://tiangeluo.github.io/">Tiange Luo*</a>, <a href="https://tiangeluo.github.io/">Chris Rockwell*</a>, <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee<sup>&#8224;</sup></a>, <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson<sup>&#8224;</sup></a> (*: equal contribution, &#8224;: equal advising)
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://arxiv.org/abs//2306.07279">NeurIPS 2023</a> </li>
                        <li class="top"><a href="https://arxiv.org/pdf/2306.07279.pdf">Paper</a> |</li>
                        <li class="top"><a href="https://github.com/crockwell/Cap3D">Code</a> |</li>
                        <li class="top"><a href="https://huggingface.co/datasets/tiange/Cap3D">Dataset</a> |</li>
			<li class="top"><a href="https://ltg-webaccess-files.s3.us-east-2.amazonaws.com/Cap3D_Scalable_3D_Captioning_with_Pretrained_Models.pptx">Slides</a> |</li>
			<li class="top"><a href="https://ltg-webaccess-files.s3.us-east-2.amazonaws.com/Cap3D_poster_ICCV.pdf">Poster (ICCV Workshop)</a> |</li>
                        <li class="top"><a href="https://tiangeluo.github.io/projectpages/bibs/cap3d.txt">BibTeX</a> </li>
                    </ul>
                </div>
            </div>
          </div>

        </div>
        <canvas id="wave-canvas"></canvas>
    </div>
    <div id="content">
        <div id="content-container" class="container">
            <div class="header"><h4><span class="indicator">=</span>Resources</h4></div>
            <div class="content">
                <ul>
                    <li>Data is hosted at <a href="https://huggingface.co/datasets/tiange/Cap3D">[Huggingface]</a>, including <strong>1,006,782</strong> descriptive captions for 3D objects in Objaverse and Objaverse-XL, associated with point clouds (16,384 colorful points), and 20 rendered images along with camera details (intrinsic & extrinsic), depth data, and masks.</li>
                    <li>Our code for captioning, rendering, and view selection are released in <a href="https://github.com/tiangeluo/DiffuRank">[Github]</a></li>
                    <li>Our code for finetuning text-to-3D models are released in <a href="https://github.com/crockwell/Cap3D/tree/main/text-to-3D">[Github]</a></li>
                    <li>Some of our fine-trained model checkpoints can be found in <a href="https://huggingface.co/datasets/tiange/Cap3D/tree/main/misc/our_finetuned_models">[Huggingface]</a></a>.</li>
                    <li>Compositional and general descriptive captions for 3D objects in the ABO dataset is at <a href="https://huggingface.co/datasets/tiange/Cap3D">[Huggingface]</a></li>
		    <li>General descriptive captions for 3D objects in the ShapeNet dataset is at <a href="https://huggingface.co/datasets/tiange/Cap3D">[Huggingface]</a></li>
                </ul>
            </div>
        </div>
        <div id="content-container" class="container">
            <div class="header"><h4><span class="indicator">=</span>Overview</h4></div>
            <div class="content">
            <style>
                .add-space {
                    margin-bottom: 20px;
                }
            </style>
            <p>
            Our experimental findings indicate that the rendering views significantly impacts the performance of 3D captioning with image-based models, such as BLIP2 and GPT4-Vision. Especially, our method, which utilizes 8 rendering views, achieves higher quality, less hallucination, and more detailed captions than GPT4-Vision with 28 views.
            </p>

            <p>
                <center>
                <img width="100%" src="https://tiangeluo.github.io/projectpages/imgs/Cap3D/DiffuRank_1.png">
                </center>
                </p>
                <!-- <p class="caption">
                <center>
                Figure 1:  Example captioning results by Cap3D.
                </center> -->
            </p>

            Both Cap3D and our newer method (DiffuRank) render input 3D objects into multiple views for caption generation (green steps). However, while Cap3D consolidates these captions into a final description (blue steps), DiffuRank employs a pre-trained text-to-3D diffusion model to identify views that better match the input objectâ€™s characteristics. These selected views are then processed by a Vision-Language Model (we used GPT4-Vision)
for captioning (orange steps).

            <p class="add-space">
                <center>
                <img width="100%" src="https://tiangeluo.github.io/projectpages/imgs/Cap3D/DiffuRank_2.png">
                </center>
                </p>
            <p>

            <p>
                Randomly sampled selected views by DiffuRank. The left row features the top-6 views as ranked by DiffuRank, while the right row displays the
                bottom-6. We adopt two different kinds of rendering, and notice that DiffuRank can select the views with the appropriate rendering that highlight object features.
            </p>
            <p>
                <center>
                <img width="100%" src="https://tiangeluo.github.io/projectpages/imgs/Cap3D/DiffuRank_3.png">
                </center>
                </p>
            </div>
        </div>

            </div>
        </div>


        <div id="content-container" class="container">
            <div class="header"><h4><span class="indicator">=</span>Related Publication</h4></div>
            <div class="content">
                <div class="paper-info", style="margin-bottom: 15px;">
                    <h3 class="title">Objaverse: A Universe of Annotated 3D Objects</h3>
                    <p class="authors_bottom">
                        Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://cvpr2023.thecvf.com/">CVPR 2023</a></a></li>
                        <li class="body"><a href="https://objaverse.allenai.org/">Project Page</a> </li>
                    </ul>
                </div>

                <div class="paper-info", style="margin-bottom: 15px;">
                    <h3 class="title">ABO: Dataset and Benchmarks for Real-World 3D Object Understanding</h3>
                    <p class="authors_bottom">
                        Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F. Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, Jitendra Malik
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://cvpr2022.thecvf.com/">CVPR 2022</a></a></li>
                        <li class="body"><a href="https://amazon-berkeley-objects.s3.amazonaws.com/index.html">Project Page</a> </li>
                    </ul>
                </div>
                <div class="paper-info", style="margin-bottom: 15px;">
                    <h3 class="title">GPT-4 Technical Report</h3>
                    <p class="authors_bottom">
                        OpenAI
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://arxiv.org/abs/2303.08774">arXiv 2023</a></a></li>
                        <li class="body"><a href="https://arxiv.org/pdf/2303.08774.pdf">Page</a> </li>
                    </ul>
                </div>
                <div class="paper-info", style="margin-bottom: 15px;">
                    <h3 class="title">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</h3>
                    <p class="authors_bottom">
                        Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://arxiv.org/abs/2301.12597">arXiv 2023</a></a></li>
                        <li class="body"><a href="https://arxiv.org/pdf/2301.12597.pdf">Page</a> </li>
                    </ul>
                </div>
                <div class="paper-info", style="margin-bottom: 15px;">
                    <h3 class="title">Objaverse-XL: A Universe of 10M+ 3D Objects</h3>
                    <p class="authors_bottom">
                        Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, Ali Farhadi
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://neurips.cc/Conferences/2023">NeurIPS 2023</a></a></li>
                        <li class="body"><a href="https://objaverse.allenai.org/">Project Page</a> </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    <div id="footer">
        <p>Thank this <a href="http://nscl.csail.mit.edu/">template</a>. <a href="https://accessibility.umich.edu/">Accessibility</a>. </p>
    </div>
    <!----------
    <div id="footer">
        <p>Thank this <a href="http://nscl.csail.mit.edu/">template</a> </p>
    </div>
    ---------->
</div>

<!-- jQuery first, then Tether, then Bootstrap JS. -->
<script src="https://code.jquery.com/jquery-3.1.1.min.js" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js"
        integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js"
        integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
<script type="text/javascript" src="static/js/jquery.color.min.js"></script>
<script type="text/javascript" src="static/js/wave.js"></script>
<script type="text/javascript">
$(function() {
    targetColor = $("#title").css("color")
    animatedLink = function(speed) {
        $(".link-li").hover(function() {
            $(this).find('.icon').animate({
                color: targetColor,
                borderColor: targetColor
            }, speed);
            $(this).find('.caption').animate({
                color: '#798350'
            })
        }, function() {
            $(this).find('.icon').animate({
                borderColor: '#cccccc',
                color: '#cccccc'
            }, speed);
            $(this).find('.caption').animate({
                color: '#cccccc'
            })
        })
    };
    // fullBg();
    animatedLink(400)
});
</script>
</body>
</html>
